version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./data:/app/data
      - ./configs:/app/configs
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-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}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o}
      - HF_MODEL_URL=${HF_MODEL_URL:-https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell}
      - HF_TOKEN=${HF_TOKEN:-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}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - REQUEST_TIMEOUT_S=${REQUEST_TIMEOUT_S:-60}
      - MAX_CONCURRENCY=${MAX_CONCURRENCY:-5}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - app-network

  frontend:
    build:
      context: .
      dockerfile: docker/Dockerfile.frontend
    ports:
      - "${FRONTEND_PORT:-7860}:7860"
    environment:
      - API_BASE_URL=http://api:8000
      - FRONTEND_PORT=7860
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  data:
    driver: local